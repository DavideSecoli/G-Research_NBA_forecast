{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBA_XGBoost_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO33YsLrwIWdUL8qyFSGqn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavideSecoli/NBA_forecast/blob/master/NBA_XGBoost_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G65G8nHcYSsz",
        "colab_type": "code",
        "outputId": "e35c0cb4-003f-45a7-fdd2-cd426016e44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://github.com/GR-NBA-data-challenge/NBA-data-template/raw/master/libupdate.py', 'libupdate.py')\n",
        "import os, datetime, argparse, requests, urllib.parse, sys, re, traceback, json\n",
        "import libupdate\n",
        "libupdate.main()\n",
        "import libsimulation\n",
        "\n",
        "import pandas as pd\n",
        "import math, numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns\n",
        "from scipy.stats import poisson,skellam\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import cv\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SFRUvvwYiqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize some settings\n",
        "settings = libsimulation.SimulationSettings()\n",
        "# This prevents you accidentally loading data beyond this point, and also defines the start of the simulation run period\n",
        "settings.cutoff = '2019-01-01'\n",
        "\n",
        "data_loader = libsimulation.NbaDataLoader(settings)\n",
        "\n",
        "data_loader.getSeasonV2('2019')\n",
        "\n",
        "#additional data sources \n",
        "get_game = data_loader.getGameV2(12481)\n",
        "get_players = data_loader.getPlayersV2('2011')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiOTHGRYeyo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to retreive data \n",
        "\n",
        "def get_multi_season_game_data(data_loader, first_year, last_year):\n",
        "    data = [pd.DataFrame(data_loader.getSeasonV2(str(season))) for season in range(first_year, last_year + 1)]\n",
        "    data = pd.concat(data, axis=0)\n",
        "    data.dropna(axis=0, inplace=True)\n",
        "    data.dateTime=pd.to_datetime(data.dateTime)\n",
        "    data.sort_values('dateTime', inplace=True)\n",
        "    data.reset_index(inplace=True, drop=True)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njS31GF0fTnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all data into tot_data \n",
        "train_data = get_multi_season_game_data(data_loader, 2015, 2018)\n",
        "test_data = get_multi_season_game_data(data_loader, 2019, 2020)\n",
        "tot_data = get_multi_season_game_data(data_loader, 2015, 2020)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dujahcFjOHRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train data features \n",
        "\n",
        "tot_data['home_blocks_perc'] = tot_data['homeBlocks'] / (tot_data['homeBlocks'] + tot_data['awayBlocks'])\n",
        "tot_data['away_blocks_perc'] = (1 - tot_data['home_blocks_perc'])\n",
        "\n",
        "tot_data['home_rebounds_perc'] = tot_data['homeRebounds'] / (tot_data['homeRebounds'] + tot_data['awayRebounds'])\n",
        "tot_data['away_rebounds_perc'] = (1 - tot_data['home_rebounds_perc'])\n",
        "\n",
        "tot_data['home_steals_perc'] = tot_data['homeSteals'] / (tot_data['homeSteals'] + tot_data['awaySteals'])\n",
        "tot_data['away_steals_perc'] = (1 - tot_data['home_steals_perc']) \n",
        "\n",
        "tot_data['home_assists_perc'] = tot_data['homeAssists'] / (tot_data['homeAssists'] + tot_data['awayAssists'])\n",
        "tot_data['away_assists_perc'] = (1 - tot_data['home_assists_perc'])\n",
        "\n",
        "# Delete columns \n",
        "tot_data = tot_data.drop(['gameId','dateTime','status','homeScore','awayScore'], axis=1) # consider droppng 'hometeamId'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPs8BVD0zt4y",
        "colab_type": "code",
        "outputId": "e935c790-edfc-4c8b-ced0-492e0c964ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "tot_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>homeTeam</th>\n",
              "      <th>homeTeamId</th>\n",
              "      <th>awayTeam</th>\n",
              "      <th>awayTeamId</th>\n",
              "      <th>pointsDiff</th>\n",
              "      <th>pointsSum</th>\n",
              "      <th>homeBlocks</th>\n",
              "      <th>homeMinutes</th>\n",
              "      <th>homeRebounds</th>\n",
              "      <th>homeSteals</th>\n",
              "      <th>homeAssists</th>\n",
              "      <th>quarter0home</th>\n",
              "      <th>quarter1home</th>\n",
              "      <th>quarter2home</th>\n",
              "      <th>quarter3home</th>\n",
              "      <th>awayBlocks</th>\n",
              "      <th>awayMinutes</th>\n",
              "      <th>awayRebounds</th>\n",
              "      <th>awaySteals</th>\n",
              "      <th>awayAssists</th>\n",
              "      <th>quarter0away</th>\n",
              "      <th>quarter1away</th>\n",
              "      <th>quarter2away</th>\n",
              "      <th>quarter3away</th>\n",
              "      <th>season</th>\n",
              "      <th>home_blocks_perc</th>\n",
              "      <th>away_blocks_perc</th>\n",
              "      <th>home_rebounds_perc</th>\n",
              "      <th>away_rebounds_perc</th>\n",
              "      <th>home_steals_perc</th>\n",
              "      <th>away_steals_perc</th>\n",
              "      <th>home_assists_perc</th>\n",
              "      <th>away_assists_perc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NO</td>\n",
              "      <td>23.0</td>\n",
              "      <td>ORL</td>\n",
              "      <td>5.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>2015</td>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>0.529915</td>\n",
              "      <td>0.470085</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.540541</td>\n",
              "      <td>0.459459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SA</td>\n",
              "      <td>24.0</td>\n",
              "      <td>DAL</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2015</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.535211</td>\n",
              "      <td>0.464789</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LAL</td>\n",
              "      <td>27.0</td>\n",
              "      <td>HOU</td>\n",
              "      <td>22.0</td>\n",
              "      <td>-18.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2015</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.433735</td>\n",
              "      <td>0.566265</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>0.578947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CHA</td>\n",
              "      <td>2.0</td>\n",
              "      <td>MIL</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>214.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2015</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.519231</td>\n",
              "      <td>0.480769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IND</td>\n",
              "      <td>13.0</td>\n",
              "      <td>PHI</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2015</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.456522</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.564103</td>\n",
              "      <td>0.435897</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  homeTeam  homeTeamId  ... home_assists_perc  away_assists_perc\n",
              "0       NO        23.0  ...          0.540541           0.459459\n",
              "1       SA        24.0  ...          0.575000           0.425000\n",
              "2      LAL        27.0  ...          0.421053           0.578947\n",
              "3      CHA         2.0  ...          0.519231           0.480769\n",
              "4      IND        13.0  ...          0.564103           0.435897\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEPQvz3keSW-",
        "colab_type": "text"
      },
      "source": [
        "**Data** **preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH4ktI9rxn2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREROCESSING \n",
        "\n",
        "# Define feature column categories by column type\n",
        "categorical_cols = [col for col in tot_data.columns if tot_data[col].dtype == 'object']\n",
        "numeric_cols = [col for col in tot_data.columns if tot_data[col].dtype != 'object']\n",
        "\n",
        "# Remove the target columns from our feature list\n",
        "numeric_cols.remove('pointsDiff')\n",
        "numeric_cols.remove('pointsSum')\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean'))\n",
        "\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # ignore set so new categories in validation set won't trigger an error post test set fit\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numerical_transformer, numeric_cols),\n",
        "        ('categorical', categorical_transformer, categorical_cols)\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t0SkIJ62laA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab target as y, remove target from X\n",
        "tot_data = tot_data.copy()\n",
        "y = tot_data['pointsSum']\n",
        "y_diff = tot_data['pointsDiff']\n",
        "X = tot_data.drop(['pointsSum','pointsDiff'], axis=1)\n",
        "\n",
        "# Split into train, test for sum \n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state = 42)\n",
        "\n",
        "# Split into train, test for diff\n",
        "train_X_diff, val_X_diff, train_y_diff, val_y_diff = train_test_split(X, y_diff, train_size=0.8, random_state = 42)\n",
        "\n",
        "# Fit the preprocessor using the training data for sum \n",
        "train_X_cleaned = preprocessor.fit_transform(train_X)\n",
        "\n",
        "# Fit the preprocessor using the training data for diff \n",
        "train_X_cleaned_diff = preprocessor.fit_transform(train_X_diff)\n",
        "\n",
        "# SUM: Run the validation set (and all future sets) through the transform without fitting again, or else you'll end up with a different pipeline!\n",
        "val_X_cleaned = preprocessor.transform(val_X)\n",
        "\n",
        "# DIFF: Run the validation set (and all future sets) through the transform without fitting again, or else you'll end up with a different pipeline!\n",
        "val_X_cleaned_diff = preprocessor.transform(val_X_diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GXKA4uMeKb-",
        "colab_type": "text"
      },
      "source": [
        "### **Tune** **XGBoost** **parameters**\n",
        "\n",
        "Train Untuned XGBoost\n",
        "Before we tune XGBoost, we first need to make sure we can run the algorithm! XGBoost uses a set of parameters to determine how quickly it converges and what shape it takes.\n",
        "\n",
        "Technical notes about XGBoost that are useful to know before digging into the algorithm:\n",
        "\n",
        "The xgboost algorithm uses DMatrix input rather than numpy arrays or pandas dataframes. A DMatrix can contain both the training X and y data (the y data is the \"label\" of a DMatrix).\n",
        "\n",
        "Different versions of gradient boosting algorithms give different names to the same tuning parameters, which can get confusing. The following names are equivalent in meaning, although different algorithms may use one or the other as the input name:\n",
        "\n",
        "**eta** = learning_rate\n",
        "\n",
        "**min_child_weight** = min_child_leaf\n",
        "\n",
        "**num_estimators** = num_boost_rounds\n",
        "\n",
        "XGBoost uses the left side terms above, whereas Scikit learn Gradient Boosting uses the right side terms above.\n",
        "\n",
        "While we'll go into more depth on each of these parameters, let's try to run XGBoost first. We'll start by saving a set of generic values for all of the parameters in a params dictionary, which we can then pass to xgb along with our training and validation data.\n",
        "\n",
        "Along with our parameters, we need to give XGBoost two pieces of information:\n",
        "\n",
        "**num_boost_round**, which is the maximum number of iterations we'll\n",
        "allow the model to compute. We can set this number high (ex. 999) and hope we never actually have to compute that many iterations, thanks to early_stopping_rounds below!\n",
        "\n",
        "**early_stopping_rounds**, which is the number of rounds (iterations) the algorithm will compute in a row without model improvement. For example, say early_stopping_rounds = 10. After every round, the algorithm checks model performance. If the model performance hasn't improved in 10 rounds, then training stops, even if we haven't gone through all num_boost_round rounds.\n",
        "\n",
        "In this first XGB run, we'll report both the best MAE and what round that MAE value occured."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YhwA2kT_Xg",
        "colab_type": "code",
        "outputId": "e5c0f31f-840b-4db5-c9a8-0e82e67f1f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Create an initial parameter list, which we'll tune as we go.\n",
        "params = {\n",
        "    'max_depth':6,\n",
        "    'min_child_weight': 1,\n",
        "    'eta':.3,\n",
        "    'subsample': 1,\n",
        "    'colsample_bytree': 1,\n",
        "    'objective':'reg:squarederror',\n",
        "    'eval_metric':'mae'\n",
        "}\n",
        "\n",
        "# Set num_boost_rounds for future use\n",
        "NUM_BOOST_ROUNDS=999\n",
        "\n",
        "# Take our inputs and format them as DMatrices for SUM\n",
        "dtrain = xgb.DMatrix(train_X_cleaned, label=train_y)\n",
        "dtest = xgb.DMatrix(val_X_cleaned, label=val_y)\n",
        "\n",
        "# Take our inputs and format them as DMatrices for DIFF\n",
        "dtrain_diff = xgb.DMatrix(train_X_cleaned_diff, label=train_y_diff)\n",
        "dtest_diff = xgb.DMatrix(val_X_cleaned_diff, label=val_y_diff)\n",
        "\n",
        "# Train XGB model \n",
        "xbg_model = xgb.train(\n",
        "    params,\n",
        "    #dtrain,\n",
        "    dtrain_diff,\n",
        "    num_boost_round=NUM_BOOST_ROUNDS,\n",
        "    #evals=[(dtest, \"Test\")],\n",
        "    evals=[(dtest_diff, \"Test\")],\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "  \n",
        "print(\"Best MAE: {:.2f}, found at round {}\".format(\n",
        "                 xbg_model.best_score,\n",
        "                 xbg_model.best_iteration))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTest-mae:9.60638\n",
            "Will train until Test-mae hasn't improved in 10 rounds.\n",
            "[1]\tTest-mae:8.61399\n",
            "[2]\tTest-mae:7.92602\n",
            "[3]\tTest-mae:7.46876\n",
            "[4]\tTest-mae:7.00312\n",
            "[5]\tTest-mae:6.56698\n",
            "[6]\tTest-mae:6.23337\n",
            "[7]\tTest-mae:5.9909\n",
            "[8]\tTest-mae:5.75291\n",
            "[9]\tTest-mae:5.59545\n",
            "[10]\tTest-mae:5.46114\n",
            "[11]\tTest-mae:5.35105\n",
            "[12]\tTest-mae:5.23061\n",
            "[13]\tTest-mae:5.12035\n",
            "[14]\tTest-mae:5.01948\n",
            "[15]\tTest-mae:4.94424\n",
            "[16]\tTest-mae:4.87662\n",
            "[17]\tTest-mae:4.8097\n",
            "[18]\tTest-mae:4.73209\n",
            "[19]\tTest-mae:4.68297\n",
            "[20]\tTest-mae:4.6342\n",
            "[21]\tTest-mae:4.57978\n",
            "[22]\tTest-mae:4.53758\n",
            "[23]\tTest-mae:4.5053\n",
            "[24]\tTest-mae:4.46482\n",
            "[25]\tTest-mae:4.43051\n",
            "[26]\tTest-mae:4.40369\n",
            "[27]\tTest-mae:4.35435\n",
            "[28]\tTest-mae:4.3158\n",
            "[29]\tTest-mae:4.28551\n",
            "[30]\tTest-mae:4.25477\n",
            "[31]\tTest-mae:4.23489\n",
            "[32]\tTest-mae:4.20252\n",
            "[33]\tTest-mae:4.17092\n",
            "[34]\tTest-mae:4.15362\n",
            "[35]\tTest-mae:4.12579\n",
            "[36]\tTest-mae:4.11036\n",
            "[37]\tTest-mae:4.09151\n",
            "[38]\tTest-mae:4.07711\n",
            "[39]\tTest-mae:4.06195\n",
            "[40]\tTest-mae:4.0371\n",
            "[41]\tTest-mae:4.02602\n",
            "[42]\tTest-mae:4.01159\n",
            "[43]\tTest-mae:3.99942\n",
            "[44]\tTest-mae:3.98792\n",
            "[45]\tTest-mae:3.97227\n",
            "[46]\tTest-mae:3.96296\n",
            "[47]\tTest-mae:3.95275\n",
            "[48]\tTest-mae:3.94671\n",
            "[49]\tTest-mae:3.93903\n",
            "[50]\tTest-mae:3.92714\n",
            "[51]\tTest-mae:3.91893\n",
            "[52]\tTest-mae:3.90905\n",
            "[53]\tTest-mae:3.90285\n",
            "[54]\tTest-mae:3.89648\n",
            "[55]\tTest-mae:3.88336\n",
            "[56]\tTest-mae:3.87738\n",
            "[57]\tTest-mae:3.87258\n",
            "[58]\tTest-mae:3.86247\n",
            "[59]\tTest-mae:3.85517\n",
            "[60]\tTest-mae:3.85025\n",
            "[61]\tTest-mae:3.8444\n",
            "[62]\tTest-mae:3.84377\n",
            "[63]\tTest-mae:3.83868\n",
            "[64]\tTest-mae:3.83743\n",
            "[65]\tTest-mae:3.83352\n",
            "[66]\tTest-mae:3.82702\n",
            "[67]\tTest-mae:3.8193\n",
            "[68]\tTest-mae:3.82325\n",
            "[69]\tTest-mae:3.82236\n",
            "[70]\tTest-mae:3.81635\n",
            "[71]\tTest-mae:3.8135\n",
            "[72]\tTest-mae:3.80667\n",
            "[73]\tTest-mae:3.80431\n",
            "[74]\tTest-mae:3.80034\n",
            "[75]\tTest-mae:3.80131\n",
            "[76]\tTest-mae:3.79542\n",
            "[77]\tTest-mae:3.79264\n",
            "[78]\tTest-mae:3.79566\n",
            "[79]\tTest-mae:3.79171\n",
            "[80]\tTest-mae:3.7893\n",
            "[81]\tTest-mae:3.78861\n",
            "[82]\tTest-mae:3.78367\n",
            "[83]\tTest-mae:3.78474\n",
            "[84]\tTest-mae:3.78534\n",
            "[85]\tTest-mae:3.78222\n",
            "[86]\tTest-mae:3.78449\n",
            "[87]\tTest-mae:3.78208\n",
            "[88]\tTest-mae:3.78041\n",
            "[89]\tTest-mae:3.77623\n",
            "[90]\tTest-mae:3.77373\n",
            "[91]\tTest-mae:3.77265\n",
            "[92]\tTest-mae:3.77036\n",
            "[93]\tTest-mae:3.76529\n",
            "[94]\tTest-mae:3.76122\n",
            "[95]\tTest-mae:3.76107\n",
            "[96]\tTest-mae:3.7605\n",
            "[97]\tTest-mae:3.75924\n",
            "[98]\tTest-mae:3.7584\n",
            "[99]\tTest-mae:3.75845\n",
            "[100]\tTest-mae:3.75817\n",
            "[101]\tTest-mae:3.75472\n",
            "[102]\tTest-mae:3.75359\n",
            "[103]\tTest-mae:3.75374\n",
            "[104]\tTest-mae:3.75278\n",
            "[105]\tTest-mae:3.75059\n",
            "[106]\tTest-mae:3.74931\n",
            "[107]\tTest-mae:3.751\n",
            "[108]\tTest-mae:3.74934\n",
            "[109]\tTest-mae:3.74655\n",
            "[110]\tTest-mae:3.74518\n",
            "[111]\tTest-mae:3.74428\n",
            "[112]\tTest-mae:3.74403\n",
            "[113]\tTest-mae:3.74451\n",
            "[114]\tTest-mae:3.74574\n",
            "[115]\tTest-mae:3.74336\n",
            "[116]\tTest-mae:3.74075\n",
            "[117]\tTest-mae:3.74094\n",
            "[118]\tTest-mae:3.7374\n",
            "[119]\tTest-mae:3.73566\n",
            "[120]\tTest-mae:3.7316\n",
            "[121]\tTest-mae:3.73047\n",
            "[122]\tTest-mae:3.72769\n",
            "[123]\tTest-mae:3.72672\n",
            "[124]\tTest-mae:3.72741\n",
            "[125]\tTest-mae:3.7274\n",
            "[126]\tTest-mae:3.72643\n",
            "[127]\tTest-mae:3.7257\n",
            "[128]\tTest-mae:3.72393\n",
            "[129]\tTest-mae:3.72486\n",
            "[130]\tTest-mae:3.72252\n",
            "[131]\tTest-mae:3.72232\n",
            "[132]\tTest-mae:3.721\n",
            "[133]\tTest-mae:3.72139\n",
            "[134]\tTest-mae:3.72094\n",
            "[135]\tTest-mae:3.72024\n",
            "[136]\tTest-mae:3.71954\n",
            "[137]\tTest-mae:3.71794\n",
            "[138]\tTest-mae:3.71798\n",
            "[139]\tTest-mae:3.71734\n",
            "[140]\tTest-mae:3.71652\n",
            "[141]\tTest-mae:3.71709\n",
            "[142]\tTest-mae:3.71646\n",
            "[143]\tTest-mae:3.71638\n",
            "[144]\tTest-mae:3.71548\n",
            "[145]\tTest-mae:3.71629\n",
            "[146]\tTest-mae:3.71659\n",
            "[147]\tTest-mae:3.71728\n",
            "[148]\tTest-mae:3.71648\n",
            "[149]\tTest-mae:3.71584\n",
            "[150]\tTest-mae:3.71412\n",
            "[151]\tTest-mae:3.71346\n",
            "[152]\tTest-mae:3.713\n",
            "[153]\tTest-mae:3.71325\n",
            "[154]\tTest-mae:3.71301\n",
            "[155]\tTest-mae:3.71334\n",
            "[156]\tTest-mae:3.71323\n",
            "[157]\tTest-mae:3.71459\n",
            "[158]\tTest-mae:3.71217\n",
            "[159]\tTest-mae:3.71229\n",
            "[160]\tTest-mae:3.71242\n",
            "[161]\tTest-mae:3.71148\n",
            "[162]\tTest-mae:3.71041\n",
            "[163]\tTest-mae:3.71018\n",
            "[164]\tTest-mae:3.7109\n",
            "[165]\tTest-mae:3.71082\n",
            "[166]\tTest-mae:3.71055\n",
            "[167]\tTest-mae:3.71025\n",
            "[168]\tTest-mae:3.71048\n",
            "[169]\tTest-mae:3.71017\n",
            "[170]\tTest-mae:3.71027\n",
            "[171]\tTest-mae:3.70993\n",
            "[172]\tTest-mae:3.71018\n",
            "[173]\tTest-mae:3.70969\n",
            "[174]\tTest-mae:3.70965\n",
            "[175]\tTest-mae:3.7093\n",
            "[176]\tTest-mae:3.70991\n",
            "[177]\tTest-mae:3.71051\n",
            "[178]\tTest-mae:3.71094\n",
            "[179]\tTest-mae:3.7104\n",
            "[180]\tTest-mae:3.71148\n",
            "[181]\tTest-mae:3.71132\n",
            "[182]\tTest-mae:3.71126\n",
            "[183]\tTest-mae:3.71101\n",
            "[184]\tTest-mae:3.71157\n",
            "[185]\tTest-mae:3.71198\n",
            "Stopping. Best iteration:\n",
            "[175]\tTest-mae:3.7093\n",
            "\n",
            "Best MAE: 3.71, found at round 175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Ef9xnfgL5S",
        "colab_type": "code",
        "outputId": "5e0c81a5-7d1c-4b05-81c2-f35cb1725893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# Calculate cross validation\n",
        "cv_results = xgb.cv(\n",
        "    params,\n",
        "    #dtrain,\n",
        "    dtrain_diff,\n",
        "    num_boost_round=NUM_BOOST_ROUNDS,\n",
        "    seed=42,\n",
        "    nfold=5,\n",
        "    metrics={'mae'},\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# Plot cross validation results\n",
        "plt.plot(cv_results['train-mae-mean'], label='train mae')\n",
        "plt.plot(cv_results['test-mae-mean'], label='test mae')\n",
        "plt.title(\"XGB Cross Validation Error\")\n",
        "plt.xlabel('Round Number')\n",
        "plt.ylabel('Mean Absoulte Error (MAE)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_36pRznn_JX",
        "colab_type": "text"
      },
      "source": [
        "## **Tuning** **max_depth** **and** **min_child_weight**\n",
        "First parameters to tune: max_depth and min_child_weight. We'll tune these concurrently using a basic grid search, meaning we'll test all combinations of the two variables in a certain range. Whichever combination has the best cross validation score, we'll pick as our values for the parameters!\n",
        "\n",
        "What do these two parameters control?\n",
        "\n",
        "**max_depth** is the largest depth allowed on any decision tree in the ensemble, where tree depth is the number of nodes from the root down to the farthest away leaf. Larger max_depth values allow for more complex trees, which means a larger chance of capturing complicated features but also a larger chance of overfitting.\n",
        "\n",
        "**min_child_weight** is a regularization factor that changes how often tree nodes split in tree creation. Only nodes with a hessian (second order partial derivative) larger than min_child_weight are allowed to split. Smaller min_child_weight values mean more complex trees, and therefore more chance of overfitting.\n",
        "As cited in the introduction, this code is partially taken from Hyperparameter tuning in XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av8swsEHiKdg",
        "colab_type": "code",
        "outputId": "c047beba-386d-4132-defd-671c96ab8630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Tune max_depth and min_child_weight\n",
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(3,8)\n",
        "    for min_child_weight in range(1,6)\n",
        "]\n",
        "\n",
        "# Define initial best params and MAE\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        #dtrain,\n",
        "        dtrain_diff,\n",
        "        num_boost_round=NUM_BOOST_ROUNDS,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mae'},\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    # Update best MAE\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].idxmin()\n",
        "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=3, min_child_weight=1\n",
            "\tMAE 2.2581966 for 580 rounds\n",
            "\n",
            "CV with max_depth=3, min_child_weight=2\n",
            "\tMAE 2.2597384 for 657 rounds\n",
            "\n",
            "CV with max_depth=3, min_child_weight=3\n",
            "\tMAE 2.3126775999999998 for 447 rounds\n",
            "\n",
            "CV with max_depth=3, min_child_weight=4\n",
            "\tMAE 2.2718046000000003 for 587 rounds\n",
            "\n",
            "CV with max_depth=3, min_child_weight=5\n",
            "\tMAE 2.2486652 for 671 rounds\n",
            "\n",
            "CV with max_depth=4, min_child_weight=1\n",
            "\tMAE 2.9133894 for 335 rounds\n",
            "\n",
            "CV with max_depth=4, min_child_weight=2\n",
            "\tMAE 2.930098 for 450 rounds\n",
            "\n",
            "CV with max_depth=4, min_child_weight=3\n",
            "\tMAE 2.9152812 for 470 rounds\n",
            "\n",
            "CV with max_depth=4, min_child_weight=4\n",
            "\tMAE 2.92448 for 357 rounds\n",
            "\n",
            "CV with max_depth=4, min_child_weight=5\n",
            "\tMAE 2.9133256 for 453 rounds\n",
            "\n",
            "CV with max_depth=5, min_child_weight=1\n",
            "\tMAE 3.4230194 for 280 rounds\n",
            "\n",
            "CV with max_depth=5, min_child_weight=2\n",
            "\tMAE 3.4148995999999996 for 267 rounds\n",
            "\n",
            "CV with max_depth=5, min_child_weight=3\n",
            "\tMAE 3.3951681999999996 for 256 rounds\n",
            "\n",
            "CV with max_depth=5, min_child_weight=4\n",
            "\tMAE 3.4418652 for 296 rounds\n",
            "\n",
            "CV with max_depth=5, min_child_weight=5\n",
            "\tMAE 3.4561280000000005 for 184 rounds\n",
            "\n",
            "CV with max_depth=6, min_child_weight=1\n",
            "\tMAE 3.9410512000000004 for 260 rounds\n",
            "\n",
            "CV with max_depth=6, min_child_weight=2\n",
            "\tMAE 3.9258448 for 258 rounds\n",
            "\n",
            "CV with max_depth=6, min_child_weight=3\n",
            "\tMAE 3.9851396 for 219 rounds\n",
            "\n",
            "CV with max_depth=6, min_child_weight=4\n",
            "\tMAE 3.9364532000000003 for 297 rounds\n",
            "\n",
            "CV with max_depth=6, min_child_weight=5\n",
            "\tMAE 3.8724718000000005 for 273 rounds\n",
            "\n",
            "CV with max_depth=7, min_child_weight=1\n",
            "\tMAE 4.4259547999999995 for 203 rounds\n",
            "\n",
            "CV with max_depth=7, min_child_weight=2\n",
            "\tMAE 4.3594016 for 271 rounds\n",
            "\n",
            "CV with max_depth=7, min_child_weight=3\n",
            "\tMAE 4.401149800000001 for 269 rounds\n",
            "\n",
            "CV with max_depth=7, min_child_weight=4\n",
            "\tMAE 4.3717276 for 283 rounds\n",
            "\n",
            "CV with max_depth=7, min_child_weight=5\n",
            "\tMAE 4.3693675999999995 for 206 rounds\n",
            "\n",
            "Best params: 3, 5, MAE: 2.2486652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mebv30Fkhro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SUM MAE: 2.5194491999999995\n",
        "#params['max_depth'] = 3\n",
        "#params['min_child_weight'] = 1\n",
        "\n",
        "#DIFF MAE: 2.2486652\n",
        "params['max_depth'] = 3\n",
        "params['min_child_weight'] = 5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAkJXIILoPoc",
        "colab_type": "text"
      },
      "source": [
        "## **Tune** **subsample** **and** **colsample**\n",
        "Great, time to move on to our next two parameters! What do these parameters control?\n",
        "\n",
        "**subsample** is the fraction of datapoints (rows of the training data) to sample each round (each tree), with 0 meaning no rows are sampled and 1 meaning all rows are sampled. The higher the value, the more likely to overfit the data.\n",
        "\n",
        "**colsample_bytree** is the fraction of features (columns of the training data) to sample each round (each tree), with 0 meaning no columns are sampled and 1 meaning all columns are sampled. The higher the value, the more likely to overfit the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaEPmosskkxt",
        "colab_type": "code",
        "outputId": "a660bdfa-cb01-49ac-a204-da96280942b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# Next, tune subsample and colsample\n",
        "gridsearch_params = [\n",
        "    (subsample, colsample)\n",
        "    for subsample in [i/10. for i in range(7,11)]\n",
        "    for colsample in [i/10. for i in range(7,11)]\n",
        "]\n",
        "\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "# We start by the largest values and go down to the smallest\n",
        "for subsample, colsample in reversed(gridsearch_params):\n",
        "    print(\"CV with subsample={}, colsample={}\".format(\n",
        "                             subsample,\n",
        "                             colsample))\n",
        "    # We update our parameters\n",
        "    params['subsample'] = subsample\n",
        "    params['colsample_bytree'] = colsample\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        #dtrain,\n",
        "        dtrain_diff,\n",
        "        num_boost_round=NUM_BOOST_ROUNDS,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mae'},\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    # Update best score\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].idxmin()\n",
        "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = (subsample,colsample)\n",
        "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with subsample=1.0, colsample=1.0\n",
            "\tMAE 2.2486652 for 671 rounds\n",
            "\n",
            "CV with subsample=1.0, colsample=0.9\n",
            "\tMAE 2.2867036 for 730 rounds\n",
            "\n",
            "CV with subsample=1.0, colsample=0.8\n",
            "\tMAE 2.2745354 for 586 rounds\n",
            "\n",
            "CV with subsample=1.0, colsample=0.7\n",
            "\tMAE 2.2736341999999996 for 593 rounds\n",
            "\n",
            "CV with subsample=0.9, colsample=1.0\n",
            "\tMAE 2.295619 for 453 rounds\n",
            "\n",
            "CV with subsample=0.9, colsample=0.9\n",
            "\tMAE 2.2423208 for 488 rounds\n",
            "\n",
            "CV with subsample=0.9, colsample=0.8\n",
            "\tMAE 2.2414351999999997 for 570 rounds\n",
            "\n",
            "CV with subsample=0.9, colsample=0.7\n",
            "\tMAE 2.2845852 for 369 rounds\n",
            "\n",
            "CV with subsample=0.8, colsample=1.0\n",
            "\tMAE 2.3289788000000002 for 497 rounds\n",
            "\n",
            "CV with subsample=0.8, colsample=0.9\n",
            "\tMAE 2.3145906 for 394 rounds\n",
            "\n",
            "CV with subsample=0.8, colsample=0.8\n",
            "\tMAE 2.3433376000000004 for 328 rounds\n",
            "\n",
            "CV with subsample=0.8, colsample=0.7\n",
            "\tMAE 2.337294 for 397 rounds\n",
            "\n",
            "CV with subsample=0.7, colsample=1.0\n",
            "\tMAE 2.438047 for 326 rounds\n",
            "\n",
            "CV with subsample=0.7, colsample=0.9\n",
            "\tMAE 2.3662204 for 376 rounds\n",
            "\n",
            "CV with subsample=0.7, colsample=0.8\n",
            "\tMAE 2.3916519999999997 for 317 rounds\n",
            "\n",
            "CV with subsample=0.7, colsample=0.7\n",
            "\tMAE 2.3399615999999996 for 506 rounds\n",
            "\n",
            "Best params: 0.9, 0.8, MAE: 2.2414351999999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyiSJ4MBlmSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SUM MAE: 2.5194491999999995\n",
        "#params['subsample'] = 1.0\n",
        "#params['colsample_bytree'] = 1.0 \n",
        "\n",
        "# DIFF MAE: 2.2414351999999997\n",
        "params['subsample'] = 0.9\n",
        "params['colsample_bytree'] = 0.8 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ynQlP8mt05",
        "colab_type": "text"
      },
      "source": [
        "# **Tune** **eta**\n",
        "Eta in XGBoost works the same way as a learning_rate, in that it controls how quickly each step changes. The smaller the eta, the smaller each step, which means the algorithm will likely take more steps to converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_vI8ZBlnfL",
        "colab_type": "code",
        "outputId": "e7460055-d49c-40b0-9d26-dd751e41d05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for eta in [.3, .2, .1, .05, .01, .005]:\n",
        "    print(\"CV with eta={}\".format(eta))\n",
        "    # We update our parameters\n",
        "    params['eta'] = eta\n",
        "    # Run and time CV\n",
        "    cv_results = xgb.cv(\n",
        "            params,\n",
        "            #dtrain,\n",
        "            dtrain_diff,\n",
        "            num_boost_round=NUM_BOOST_ROUNDS,\n",
        "            seed=42,\n",
        "            nfold=5,\n",
        "            metrics=['mae'],\n",
        "            early_stopping_rounds=10\n",
        "          )\n",
        "    # Update best score\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].idxmin()\n",
        "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = eta\n",
        "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with eta=0.3\n",
            "\tMAE 2.2414351999999997 for 570 rounds\n",
            "\n",
            "CV with eta=0.2\n",
            "\tMAE 2.0510684 for 818 rounds\n",
            "\n",
            "CV with eta=0.1\n",
            "\tMAE 1.917233 for 997 rounds\n",
            "\n",
            "CV with eta=0.05\n",
            "\tMAE 2.1662568 for 998 rounds\n",
            "\n",
            "CV with eta=0.01\n",
            "\tMAE 4.2836232 for 998 rounds\n",
            "\n",
            "CV with eta=0.005\n",
            "\tMAE 5.5378913999999995 for 998 rounds\n",
            "\n",
            "Best params: 0.1, MAE: 1.917233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKOsfbdBlrR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SUM MAE: 2.0395146000000004\n",
        "sum_param = {'colsample_bytree': 1.0,\n",
        "             'eta': 0.05,\n",
        "             'eval_metric': 'mae',\n",
        "             'max_depth': 3,\n",
        "             'min_child_weight': 1,\n",
        "             'objective': 'reg:squarederror',\n",
        "             'subsample': 1.0}\n",
        "  \n",
        "# DIFF MAE: 1.917233; 997 rounds \n",
        "diff_param = {'colsample_bytree': 0.8,\n",
        "              'eta': 0.1,\n",
        "              'eval_metric': 'mae',\n",
        "              'max_depth': 3,\n",
        "              'min_child_weight': 5,\n",
        "              'objective': 'reg:squarederror',\n",
        "              'subsample': 0.9}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GRHSD7j4cGf",
        "colab_type": "code",
        "outputId": "cf84d7b1-2f87-4b16-c2e3-c0c3b7281733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "# Put together a final model, using the parameters found in tuning\n",
        "\n",
        "final_model = xgb.XGBRegressor(n_estimators=986, \n",
        "                               learning_rate=0.05, \n",
        "                               max_depth = 3, \n",
        "                               min_child_weight=1, \n",
        "                               subsample=1.0, \n",
        "                               colsample_bytree=1.0)\n",
        "\n",
        "final_model_diff = XGBRegressor(n_estimators=997, \n",
        "                                learning_rate=0.1, \n",
        "                                max_depth = 3, \n",
        "                                min_child_weight=5, \n",
        "                                subsample=0.9, \n",
        "                                colsample_bytree=0.8)\n",
        "\n",
        "# Pipeline SUM\n",
        "final_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', final_model)])\n",
        "\n",
        "# Pipeline DIFF\n",
        "final_pipeline_diff = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', final_model_diff)])\n",
        "\n",
        "# Preprocessing of validation data, get predictions for SUM \n",
        "final_pipeline.fit(train_X,train_y)\n",
        "test_data_labels = final_pipeline.predict(val_X)\n",
        "train_data_labels = final_pipeline.predict(train_X)\n",
        "\n",
        "# Preprocessing of validation data, get predictions for DIFF\n",
        "final_pipeline_diff.fit(train_X_diff,train_y_diff)\n",
        "test_data_labels_diff = final_pipeline_diff.predict(val_X_diff)\n",
        "train_data_labels_diff = final_pipeline_diff.predict(train_X_diff)\n",
        "\n",
        "# Create predictions to be submitted!\n",
        "results = pd.DataFrame({'homeTeam': val_X['homeTeam'],'awayTeam': val_X['awayTeam'],\n",
        "                        'pointsSum': val_y,'predictedPointsSum': test_data_labels,\n",
        "                        'pointsDiff': val_y_diff, 'predictedPointsDiff': test_data_labels_diff}) \n",
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16:43:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[16:43:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>homeTeam</th>\n",
              "      <th>awayTeam</th>\n",
              "      <th>pointsSum</th>\n",
              "      <th>predictedPointsSum</th>\n",
              "      <th>pointsDiff</th>\n",
              "      <th>predictedPointsDiff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5360</th>\n",
              "      <td>NY</td>\n",
              "      <td>PHO</td>\n",
              "      <td>238.0</td>\n",
              "      <td>236.738419</td>\n",
              "      <td>-18.0</td>\n",
              "      <td>-17.269978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>POR</td>\n",
              "      <td>DEN</td>\n",
              "      <td>234.0</td>\n",
              "      <td>234.135117</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.409918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5047</th>\n",
              "      <td>DEN</td>\n",
              "      <td>UTA</td>\n",
              "      <td>191.0</td>\n",
              "      <td>190.419434</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.469049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3617</th>\n",
              "      <td>SA</td>\n",
              "      <td>MEM</td>\n",
              "      <td>184.0</td>\n",
              "      <td>194.466232</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.102319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2921</th>\n",
              "      <td>HOU</td>\n",
              "      <td>PHO</td>\n",
              "      <td>246.0</td>\n",
              "      <td>242.945694</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.403168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>PHO</td>\n",
              "      <td>SAC</td>\n",
              "      <td>208.0</td>\n",
              "      <td>209.619568</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>-4.983862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5247</th>\n",
              "      <td>WAS</td>\n",
              "      <td>BKN</td>\n",
              "      <td>190.0</td>\n",
              "      <td>192.228195</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9.607851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>DAL</td>\n",
              "      <td>LAC</td>\n",
              "      <td>226.0</td>\n",
              "      <td>222.833450</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.791741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5158</th>\n",
              "      <td>CHA</td>\n",
              "      <td>BOS</td>\n",
              "      <td>229.0</td>\n",
              "      <td>230.469910</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.992653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4792</th>\n",
              "      <td>MIN</td>\n",
              "      <td>MEM</td>\n",
              "      <td>194.0</td>\n",
              "      <td>196.945129</td>\n",
              "      <td>-8.0</td>\n",
              "      <td>-8.076322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1093 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     homeTeam awayTeam  ...  pointsDiff  predictedPointsDiff\n",
              "5360       NY      PHO  ...       -18.0           -17.269978\n",
              "1086      POR      DEN  ...         6.0             4.409918\n",
              "5047      DEN      UTA  ...        15.0            15.469049\n",
              "3617       SA      MEM  ...         6.0             2.102319\n",
              "2921      HOU      PHO  ...        16.0            16.403168\n",
              "...       ...      ...  ...         ...                  ...\n",
              "3467      PHO      SAC  ...        -6.0            -4.983862\n",
              "5247      WAS      BKN  ...        14.0             9.607851\n",
              "1344      DAL      LAC  ...        10.0             9.791741\n",
              "5158      CHA      BOS  ...         5.0             3.992653\n",
              "4792      MIN      MEM  ...        -8.0            -8.076322\n",
              "\n",
              "[1093 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQvyWzSmHu2x",
        "colab_type": "code",
        "outputId": "11ca001c-d9bc-46ea-dc86-c78f237eef58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sum_mean = results['pointsSum'].mean()\n",
        "diff_mean = results['pointsDiff'].mean()\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(train_y,train_data_labels))\n",
        "print(\"In sample SUM RMSE: %f\" % (rmse_train))\n",
        "\n",
        "rmse_train_diff = np.sqrt(mean_squared_error(train_y_diff,train_data_labels_diff))\n",
        "print(\"In sample DIFF RMSE: %f\" % (rmse_train_diff))\n",
        "\n",
        "rmse_test = np.sqrt(mean_squared_error(val_y, test_data_labels))\n",
        "print(\"Out of sample SUM RMSE: %f\" % (rmse_test))\n",
        "\n",
        "rmse_test_diff = np.sqrt(mean_squared_error(val_y_diff, test_data_labels_diff))\n",
        "print(\"Out of sample DIFF RMSE: %f\" % (rmse_test_diff))\n",
        "\n",
        "train_accuracy = (1 - rmse_train / sum_mean) * 100\n",
        "test_accuracy = (1 - rmse_test / sum_mean) * 100\n",
        "\n",
        "print('In sample SUM accuracy is :', train_accuracy)\n",
        "\n",
        "#In sample SUM RMSE: 3.507907\n",
        "#In sample DIFF RMSE: 4.407704\n",
        "#Out of sample SUM RMSE: 4.592524\n",
        "#Out of sample DIFF RMSE: 5.328730\n",
        "#In sample SUM accuracy is : 98.31852806530735"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In sample SUM RMSE: 1.595290\n",
            "In sample DIFF RMSE: 0.976098\n",
            "Out of sample SUM RMSE: 2.699398\n",
            "Out of sample DIFF RMSE: 2.325618\n",
            "In sample SUM accuracy is : 99.23531769127149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PHmmwUOq_9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write some code\n",
        "def predict(required_predictions, data_loader, log=lambda x: print(x)):\n",
        "    first_year = 2016\n",
        "    \n",
        "    log('Loading training data')\n",
        "    train_data = get_multi_season_game_data(data_loader, first_year=first_year, last_year=2020)\n",
        "    \n",
        "    log('Implementing new features on train data')\n",
        "\n",
        "    # Train data features \n",
        "\n",
        "    train_data['home_blocks_perc'] = train_data['homeBlocks'] / (train_data['homeBlocks'] + train_data['awayBlocks'])\n",
        "    train_data['away_blocks_perc'] = (1 - train_data['home_blocks_perc'])\n",
        "    train_data['home_rebounds_perc'] = train_data['homeRebounds'] / (train_data['homeRebounds'] + train_data['awayRebounds'])\n",
        "    train_data['away_rebounds_perc'] = (1 - train_data['home_rebounds_perc'])\n",
        "    train_data['home_steals_perc'] = train_data['homeSteals'] / (train_data['homeSteals'] + train_data['awaySteals'])\n",
        "    train_data['away_steals_perc'] = (1 - train_data['home_steals_perc'])\n",
        "    train_data['home_assists_perc'] = train_data['homeAssists'] / (train_data['homeAssists'] + train_data['awayAssists'])\n",
        "    train_data['away_assists_perc'] = (1 - train_data['home_assists_perc'])\n",
        "\n",
        "    # Delete columns \n",
        "    train_data = train_data.drop(['gameId','dateTime','status','homeScore','awayScore'], axis=1) # consider droppng 'hometeamId'\n",
        "    print(train_data.shape)\n",
        "\n",
        "    # Out of sample data \n",
        "    test = train_data[:300]\n",
        "    # In sample data\n",
        "    train_data = train_data[300:]\n",
        "\n",
        "    print(test.shape)\n",
        "\n",
        "    # DATA PREROCESSING\n",
        "\n",
        "    # Define feature column categories by column type for train data\n",
        "    numeric_cols = [col for col in train_data.columns if train_data[col].dtype != 'object']\n",
        "    categorical_cols = [col for col in train_data.columns if train_data[col].dtype == 'object']\n",
        "\n",
        "\n",
        "    # Define feature column categories by column type for test data\n",
        "    numeric_cols_test = [col for col in test.columns if test[col].dtype != 'object']\n",
        "    categorical_cols_test = [col for col in test.columns if test[col].dtype == 'object']\n",
        "\n",
        "\n",
        "    # Remove the target columns from our feature list\n",
        "    numeric_cols.remove('pointsDiff')\n",
        "    numeric_cols.remove('pointsSum')\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('scaler', MinMaxScaler()),\n",
        "        ('imputer', SimpleImputer(strategy='mean'))\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')) # ignore set so new categories in validation set won't trigger an error post test set fit\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical training data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('numeric', numerical_transformer, numeric_cols),\n",
        "            ('categorical', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "    \n",
        "    # Grab target as y, remove target from X\n",
        "    train_data = train_data.copy()\n",
        "    train_y = train_data['pointsSum']\n",
        "    train_y_diff = train_data['pointsDiff']\n",
        "    train_X = train_data.drop(['pointsSum','pointsDiff'], axis=1)\n",
        "    train_X_diff = train_data.drop(['pointsSum','pointsDiff'], axis=1)\n",
        "    test_y = test['pointsSum']\n",
        "    test_y_diff = test['pointsDiff']\n",
        "    test_X = test.drop(['pointsSum','pointsDiff'], axis=1)\n",
        "\n",
        "    # Fit the preprocessor using the training data X\n",
        "    #train_X_cleaned = preprocessor.fit_transform(train_X)\n",
        "\n",
        "    # Fit the preprocessor using the training data for diff \n",
        "    #train_X_cleaned_dff = preprocessor.fit_transform(train_X_diff)\n",
        "\n",
        "    # Run the validation set (and all future sets) through the transform without fitting again, or else you'll end up with a different pipeline!\n",
        "    #val_X_cleaned = preprocessor.transform(val_X)\n",
        "\n",
        "    # DIFF: Run the validation set (and all future sets) through the transform without fitting again, or else you'll end up with a different pipeline!\n",
        "    #val_X_cleaned = preprocessor.transform(val_X_diff)\n",
        "\n",
        "    log('Fitting XGBoostRegression model')\n",
        "\n",
        "    # Put together a final model, using the parameters found in tuning\n",
        "\n",
        "    final_model = XGBRegressor(n_estimators=986, \n",
        "                               learning_rate=0.05, \n",
        "                               max_depth = 3, \n",
        "                               min_child_weight=1, \n",
        "                               subsample=1.0, \n",
        "                               colsample_bytree=1.0)\n",
        "\n",
        "    final_model_diff = XGBRegressor(n_estimators=997, \n",
        "                                    learning_rate=0.1, \n",
        "                                    max_depth = 3, \n",
        "                                    min_child_weight=5, \n",
        "                                    subsample=0.9, \n",
        "                                    colsample_bytree=0.8)\n",
        "\n",
        "    final_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                    ('model', final_model)])\n",
        "\n",
        "    final_pipeline_diff = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                    ('model', final_model_diff)])\n",
        "    # Fit train for sum  \n",
        "    final_pipeline_sum = final_pipeline.fit(train_X,train_y)\n",
        "\n",
        "    # Fit train for diff\n",
        "    final_pipeline_diff = final_pipeline_diff.fit(train_X_diff,train_y_diff)\n",
        "\n",
        "    log('Generating predictions')\n",
        "\n",
        "    test['predictedSum'] = final_pipeline_sum.predict(test_X)\n",
        "    test['predictedDiff'] = final_pipeline_diff.predict(test_X)\n",
        "    \n",
        "    required_predictions['predictedDiff'] = test['predictedDiff']\n",
        "    required_predictions['predictedSum'] = test['predictedSum']\n",
        "\n",
        "    results = pd.DataFrame({'homeTeam': test['homeTeam'],'awayTeam': test['awayTeam'],\n",
        "                            'pointsSum': test_y,'predictedPointsSum': test['predictedSum'],\n",
        "                            'pointsDiff': test_y_diff, 'predictedPointsDiff': test['predictedDiff']})\n",
        "\n",
        "    rmse_train = np.sqrt(mean_squared_error(test_y,test['predictedSum']))\n",
        "    print(\"Out of sample SUM RMSE: %f\" % (rmse_train))\n",
        "\n",
        "    rmse_train_diff = np.sqrt(mean_squared_error(test_y_diff,test['predictedDiff']))\n",
        "    print(\"Out of sample DIFF RMSE: %f\" % (rmse_train_diff))\n",
        "    \n",
        "    log('Finished')\n",
        "    \n",
        "#     return required_predictions.to_dict('records')\n",
        "    return required_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01VonOdbrtIw",
        "colab_type": "code",
        "outputId": "a51dfc2d-6985-4e59-ab2a-cb05a40c6be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "required_predictions = test_data[:300][['homeTeam', 'awayTeam', 'dateTime', 'gameId']]\n",
        "required_predictions = predict(required_predictions, data_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data\n",
            "Implementing new features on train data\n",
            "(4238, 33)\n",
            "(300, 33)\n",
            "Fitting XGBoostRegression model\n",
            "[16:47:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[16:48:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Generating predictions\n",
            "Out of sample SUM RMSE: 2.571294\n",
            "Out of sample DIFF RMSE: 2.340667\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nZW7L3Y1abQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}